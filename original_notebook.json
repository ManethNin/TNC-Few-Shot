{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530dec66",
   "metadata": {},
   "source": [
    "# TNC Original Codebase Classification Evaluation\n",
    "\n",
    "This notebook implements the **exact same** classification evaluation as used in the original TNC codebase. It uses the WFClassificationExperiment class approach with the original WFClassifier model.\n",
    "\n",
    "## Approach\n",
    "- Uses original `WFEncoder` and `WFClassifier` from the codebase\n",
    "- Follows the exact data preprocessing and windowing approach\n",
    "- Uses the same training loop and metrics (accuracy, AUC, AUPRC)\n",
    "- Implements the original evaluation framework for fair comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef11281",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02667884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up paths to your saved checkpoint, data, and plots folders\n",
    "DRIVE_PATH = '/content/drive/MyDrive'  # Adjust this path as needed\n",
    "CHECKPOINT_PATH = os.path.join(DRIVE_PATH, 'ckpt')\n",
    "DATA_PATH = os.path.join(DRIVE_PATH, 'data')\n",
    "PLOTS_PATH = os.path.join(DRIVE_PATH, 'plots')\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "os.makedirs(PLOTS_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Plots path: {PLOTS_PATH}\")\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Checkpoint exists: {os.path.exists(CHECKPOINT_PATH)}\")\n",
    "print(f\"Data exists: {os.path.exists(DATA_PATH)}\")\n",
    "print(f\"Plots exists: {os.path.exists(PLOTS_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1921c01",
   "metadata": {},
   "source": [
    "## 2. Import Original Libraries and Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries exactly as in original codebase\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, average_precision_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22371dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define original model classes exactly as in the codebase\n",
    "\n",
    "class WFEncoder(nn.Module):\n",
    "    \"\"\"Original WFEncoder from TNC codebase\"\"\"\n",
    "    def __init__(self, encoding_size=64, classify=False, n_classes=None):\n",
    "        super(WFEncoder, self).__init__()\n",
    "        \n",
    "        self.encoding_size = encoding_size\n",
    "        self.n_classes = n_classes\n",
    "        self.classify = classify\n",
    "        self.classifier = None\n",
    "        \n",
    "        if self.classify:\n",
    "            if self.n_classes is None:\n",
    "                raise ValueError('Need to specify the number of output classes for the encoder')\n",
    "            else:\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(self.encoding_size, self.n_classes)\n",
    "                )\n",
    "                nn.init.xavier_uniform_(self.classifier[1].weight)\n",
    "\n",
    "        # Original convolutional layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(2, 64, kernel_size=4, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(64, eps=0.001),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(64, eps=0.001),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(128, eps=0.001),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(128, eps=0.001),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(256, eps=0.001),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(256, eps=0.001),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Original fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(79872, 2048),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(2048, eps=0.001),\n",
    "            nn.Linear(2048, self.encoding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        encoding = self.fc(x)\n",
    "        \n",
    "        if self.classify:\n",
    "            c = self.classifier(encoding)\n",
    "            return c\n",
    "        else:\n",
    "            return encoding\n",
    "\n",
    "\n",
    "class WFClassifier(torch.nn.Module):\n",
    "    \"\"\"Original WFClassifier from TNC codebase\"\"\"\n",
    "    def __init__(self, encoding_size, output_size):\n",
    "        super(WFClassifier, self).__init__()\n",
    "        self.encoding_size = encoding_size\n",
    "        self.output_size = output_size\n",
    "        self.classifier = nn.Linear(self.encoding_size, output_size)\n",
    "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c = self.classifier(x)\n",
    "        return c\n",
    "\n",
    "print(\"Original TNC models defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f9027",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained TNC Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494dfcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained TNC encoder exactly as in original codebase\n",
    "data = 'waveform'\n",
    "cv = 0  # Cross-validation index\n",
    "encoding_size = 64\n",
    "n_classes = 4\n",
    "\n",
    "checkpoint_file = os.path.join(CHECKPOINT_PATH, data, f'checkpoint_{cv}.pth.tar')\n",
    "\n",
    "print(f\"Loading checkpoint from: {checkpoint_file}\")\n",
    "print(f\"Checkpoint exists: {os.path.exists(checkpoint_file)}\")\n",
    "\n",
    "if not os.path.exists(checkpoint_file):\n",
    "    print(\"ERROR: Checkpoint file not found!\")\n",
    "    print(f\"Make sure your checkpoint is saved as: ckpt/{data}/checkpoint_{cv}.pth.tar\")\n",
    "    print(\"Available files in checkpoint directory:\")\n",
    "    if os.path.exists(os.path.join(CHECKPOINT_PATH, data)):\n",
    "        print(os.listdir(os.path.join(CHECKPOINT_PATH, data)))\n",
    "    else:\n",
    "        print(f\"{data} directory doesn't exist\")\n",
    "else:\n",
    "    # Load checkpoint exactly as in original WFClassificationExperiment\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "    print(f\"Checkpoint loaded successfully!\")\n",
    "    print(f\"Available keys in checkpoint: {list(checkpoint.keys())}\")\n",
    "    \n",
    "    # Initialize encoder exactly as in original code\n",
    "    encoder = WFEncoder(encoding_size=encoding_size)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    encoder = encoder.to(device)\n",
    "    encoder.eval()\n",
    "    \n",
    "    # Initialize classifier exactly as in original code\n",
    "    classifier = WFClassifier(encoding_size=encoding_size, output_size=n_classes).to(device)\n",
    "    \n",
    "    # Also create e2e model for comparison (as in original)\n",
    "    e2e_model = WFEncoder(encoding_size=encoding_size, classify=True, n_classes=n_classes).to(device)\n",
    "    \n",
    "    print(\"Models initialized exactly as in original codebase!\")\n",
    "    print(f\"Encoder device: {next(encoder.parameters()).device}\")\n",
    "    print(f\"Classifier device: {next(classifier.parameters()).device}\")\n",
    "    \n",
    "    # Print checkpoint info if available\n",
    "    if 'best_accuracy' in checkpoint:\n",
    "        print(f\"Best training accuracy: {checkpoint['best_accuracy']:.3f}\")\n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"Training epoch: {checkpoint['epoch']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff0ca8",
   "metadata": {},
   "source": [
    "## 4. Load and Preprocess Data (Original Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data exactly as in original WFClassificationExperiment\n",
    "window_size = 2500  # Original window size\n",
    "\n",
    "wf_datapath = os.path.join(DATA_PATH, 'waveform_data', 'processed')\n",
    "\n",
    "# Check if data files exist\n",
    "x_train_file = os.path.join(wf_datapath, 'x_train.pkl')\n",
    "y_train_file = os.path.join(wf_datapath, 'state_train.pkl')\n",
    "\n",
    "print(f\"Data directory: {wf_datapath}\")\n",
    "print(f\"x_train exists: {os.path.exists(x_train_file)}\")\n",
    "print(f\"y_train exists: {os.path.exists(y_train_file)}\")\n",
    "\n",
    "# Load data exactly as in original codebase\n",
    "try:\n",
    "    with open(x_train_file, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    with open(y_train_file, 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "    \n",
    "    print(f\"Original data loaded successfully!\")\n",
    "    print(f\"x shape: {x.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "    # Data preprocessing exactly as in original WFClassificationExperiment\n",
    "    T = x.shape[-1]\n",
    "    x_window = np.split(x[:, :, :window_size * (T // window_size)], (T//window_size), -1)\n",
    "    y_window = np.concatenate(np.split(y[:, :window_size * (T // window_size)], (T // window_size), -1), 0).astype(int)\n",
    "    y_window = torch.Tensor(np.array([np.bincount(yy).argmax() for yy in y_window]))\n",
    "    \n",
    "    # Shuffle exactly as in original code\n",
    "    shuffled_inds = list(range(len(y_window)))\n",
    "    random.shuffle(shuffled_inds)\n",
    "    x_window = torch.Tensor(np.concatenate(x_window, 0))\n",
    "    x_window = x_window[shuffled_inds]\n",
    "    y_window = y_window[shuffled_inds]\n",
    "    \n",
    "    # Split exactly as in original (60% train, 40% validation)\n",
    "    n_train = int(0.6*len(x_window))\n",
    "    trainset = torch.utils.data.TensorDataset(x_window[:n_train], y_window[:n_train])\n",
    "    validset = torch.utils.data.TensorDataset(x_window[n_train:], y_window[n_train:])\n",
    "    \n",
    "    # Create dataloaders exactly as in original\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(validset, batch_size=100, shuffle=True)\n",
    "    \n",
    "    print(f\"Windowed data shape: {x_window.shape}\")\n",
    "    print(f\"Windowed labels shape: {y_window.shape}\")\n",
    "    print(f\"Number of classes: {len(torch.unique(y_window))}\")\n",
    "    print(f\"Training samples: {n_train}\")\n",
    "    print(f\"Validation samples: {len(x_window) - n_train}\")\n",
    "    print(f\"Class distribution: {torch.bincount(y_window.long())}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please check your data file paths and formats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756055d",
   "metadata": {},
   "source": [
    "## 5. Original Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions exactly as in original codebase\n",
    "\n",
    "def _train_tnc_classifier(encoder, classifier, train_loader, lr):\n",
    "    \"\"\"Exact copy of _train_tnc_classifier from original evaluations.py\"\"\"\n",
    "    classifier.train()\n",
    "    encoder.eval()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "    epoch_loss, epoch_auc = 0, 0\n",
    "    epoch_acc = 0\n",
    "    batch_count = 0\n",
    "    y_all, prediction_all = [], []\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        if i > 30:  # Original limit from codebase\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        encodings = encoder(x)\n",
    "        prediction = classifier(encodings)\n",
    "        state_prediction = torch.argmax(prediction, dim=1)\n",
    "        loss = loss_fn(prediction, y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_all.append(y.cpu())\n",
    "        prediction_all.append(prediction.detach().cpu().numpy())\n",
    "\n",
    "        epoch_acc += torch.eq(state_prediction, y).sum().item()/len(x)\n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "    y_all = np.concatenate(y_all, 0)\n",
    "    prediction_all = np.concatenate(prediction_all, 0)\n",
    "    prediction_class_all = np.argmax(prediction_all, -1)\n",
    "    y_onehot_all = np.zeros(prediction_all.shape)\n",
    "    y_onehot_all[np.arange(len(y_onehot_all)), y_all.astype(int)] = 1\n",
    "    epoch_auc = roc_auc_score(y_onehot_all, prediction_all)\n",
    "    epoch_auprc = average_precision_score(y_onehot_all, prediction_all)\n",
    "    c = confusion_matrix(y_all.astype(int), prediction_class_all)\n",
    "    return epoch_loss / batch_count, epoch_acc / batch_count, epoch_auc, epoch_auprc, c\n",
    "\n",
    "\n",
    "def _test_model(model, valid_loader):\n",
    "    \"\"\"Exact copy of _test function from original evaluations.py\"\"\"\n",
    "    model.eval()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    epoch_loss, epoch_auc = 0, 0\n",
    "    epoch_acc = 0\n",
    "    batch_count = 0\n",
    "    y_all, prediction_all = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            prediction = model(x)\n",
    "            state_prediction = torch.argmax(prediction, -1)\n",
    "            loss = loss_fn(prediction, y.long())\n",
    "            y_all.append(y.cpu())\n",
    "            prediction_all.append(prediction.detach().cpu().numpy())\n",
    "\n",
    "            epoch_acc += torch.eq(state_prediction, y).sum().item()/len(x)\n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "    y_all = np.concatenate(y_all, 0)\n",
    "    prediction_all = np.concatenate(prediction_all, 0)\n",
    "    y_onehot_all = np.zeros(prediction_all.shape)\n",
    "    prediction_class_all = np.argmax(prediction_all, -1)\n",
    "    y_onehot_all[np.arange(len(y_onehot_all)), y_all.astype(int)] = 1\n",
    "    epoch_auc = roc_auc_score(y_onehot_all, prediction_all)\n",
    "    epoch_auprc = average_precision_score(y_onehot_all, prediction_all)\n",
    "    c = confusion_matrix(y_all.astype(int), prediction_class_all)\n",
    "    return epoch_loss / batch_count, epoch_acc / batch_count, epoch_auc, epoch_auprc, c\n",
    "\n",
    "print(\"Original training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989ca4d",
   "metadata": {},
   "source": [
    "## 6. Run Original Classification Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment exactly as in original codebase\n",
    "n_epochs = 8  # Original value for waveform data\n",
    "lr_cls = 0.01  # Original learning rate for classifier\n",
    "\n",
    "print(\"Starting TNC classification training (original method)...\")\n",
    "\n",
    "# Track metrics exactly as in original run() function\n",
    "tnc_acc, tnc_loss, tnc_auc, tnc_auprc = [], [], [], []\n",
    "tnc_acc_test, tnc_loss_test, tnc_auc_test, tnc_auprc_test = [], [], [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train TNC classifier (frozen encoder + trainable classifier)\n",
    "    loss, acc, auc, auprc, _ = _train_tnc_classifier(encoder, classifier, train_loader, lr_cls)\n",
    "    tnc_acc.append(acc)\n",
    "    tnc_loss.append(loss)\n",
    "    tnc_auc.append(auc)\n",
    "    tnc_auprc.append(auprc)\n",
    "    \n",
    "    # Test on validation set\n",
    "    loss, acc, auc, auprc, c_mtx_enc = _test_model(torch.nn.Sequential(encoder, classifier), valid_loader)\n",
    "    tnc_acc_test.append(acc)\n",
    "    tnc_loss_test.append(loss)\n",
    "    tnc_auc_test.append(auc)\n",
    "    tnc_auprc_test.append(auprc)\n",
    "\n",
    "    # Print progress exactly as in original (every 5 epochs, but we have 8 total)\n",
    "    if epoch % 5 == 0 or epoch == n_epochs - 1:\n",
    "        print('***** Epoch %d *****' % epoch)\n",
    "        print('TNC =====> Training Loss: %.3f \\\\t Training Acc: %.3f \\\\t Training AUC: %.3f \\\\t Training AUPRC: %.3f'\n",
    "              '\\\\t Test Loss: %.3f \\\\t Test Acc: %.3f \\\\t Test AUC: %.3f \\\\t Test AUPRC: %.3f'\n",
    "              % (tnc_loss[-1], tnc_acc[-1], tnc_auc[-1], tnc_auprc[-1], \n",
    "                 tnc_loss_test[-1], tnc_acc_test[-1], tnc_auc_test[-1], tnc_auprc_test[-1]))\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ FINAL RESULTS (Original TNC Method)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ… Final Test Accuracy: {tnc_acc_test[-1]:.4f} ({tnc_acc_test[-1]*100:.2f}%)\")\n",
    "print(f\"ðŸ“ˆ Final Test AUPRC: {tnc_auprc_test[-1]:.4f}\")\n",
    "print(f\"ðŸ”„ Final Test AUC: {tnc_auc_test[-1]:.4f}\")\n",
    "print(f\"ðŸ“‰ Final Test Loss: {tnc_loss_test[-1]:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d68c2f",
   "metadata": {},
   "source": [
    "## 7. Original Visualization and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots exactly as in original run() function\n",
    "\n",
    "# Create plots directory if needed\n",
    "plots_dir = os.path.join(PLOTS_PATH, data)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# 1. Accuracy trend plot (exactly as in original)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(n_epochs), tnc_acc, label=\"TNC train\", linewidth=2)\n",
    "plt.plot(np.arange(n_epochs), tnc_acc_test, label=\"TNC test\", linewidth=2)\n",
    "plt.title(\"Accuracy trend for the TNC model (Original Method)\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(plots_dir, f\"classification_accuracy_comparison_{cv}.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. AUC trend plot (exactly as in original)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(n_epochs), tnc_auc, label=\"TNC train\", linewidth=2)\n",
    "plt.plot(np.arange(n_epochs), tnc_auc_test, label=\"TNC test\", linewidth=2)\n",
    "plt.title(\"AUC trend for the TNC model (Original Method)\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(plots_dir, f\"classification_auc_comparison_{cv}.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. AUPRC trend plot (added for completeness)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(n_epochs), tnc_auprc, label=\"TNC train\", linewidth=2)\n",
    "plt.plot(np.arange(n_epochs), tnc_auprc_test, label=\"TNC test\", linewidth=2)\n",
    "plt.title(\"AUPRC trend for the TNC model (Original Method)\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUPRC\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(plots_dir, f\"classification_auprc_comparison_{cv}.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 4. Confusion matrix exactly as in original\n",
    "df_cm = pd.DataFrame(c_mtx_enc, index=[i for i in ['']*n_classes],\n",
    "                     columns=[i for i in ['']*n_classes])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='d')\n",
    "plt.title(\"TNC Encoder Confusion Matrix (Original Method)\", fontsize=14)\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.savefig(os.path.join(plots_dir, \"encoder_cf_matrix.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\\\nðŸ“Š All plots saved to: {plots_dir}\")\n",
    "print(f\"ðŸ“ˆ Accuracy plot: classification_accuracy_comparison_{cv}.png\")\n",
    "print(f\"ðŸ“ˆ AUC plot: classification_auc_comparison_{cv}.png\") \n",
    "print(f\"ðŸ“ˆ AUPRC plot: classification_auprc_comparison_{cv}.png\")\n",
    "print(f\"ðŸŽ¯ Confusion matrix: encoder_cf_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results summary exactly as needed\n",
    "results_summary = {\n",
    "    'model': 'TNC_Original_Classification',\n",
    "    'method': 'WFClassificationExperiment (Original Codebase)',\n",
    "    'encoder_checkpoint': checkpoint_file,\n",
    "    'data_type': data,\n",
    "    'cv': cv,\n",
    "    'n_epochs': n_epochs,\n",
    "    'learning_rate': lr_cls,\n",
    "    'encoding_size': encoding_size,\n",
    "    'n_classes': n_classes,\n",
    "    'window_size': window_size,\n",
    "    'training_samples': n_train,\n",
    "    'validation_samples': len(x_window) - n_train,\n",
    "    \n",
    "    # Final metrics (exactly as original returns)\n",
    "    'final_test_accuracy': float(tnc_acc_test[-1]),\n",
    "    'final_test_auc': float(tnc_auc_test[-1]),\n",
    "    'final_test_auprc': float(tnc_auprc_test[-1]),\n",
    "    'final_test_loss': float(tnc_loss_test[-1]),\n",
    "    \n",
    "    # Training progression\n",
    "    'training_accuracy_progression': [float(x) for x in tnc_acc],\n",
    "    'training_auc_progression': [float(x) for x in tnc_auc],\n",
    "    'training_auprc_progression': [float(x) for x in tnc_auprc],\n",
    "    'test_accuracy_progression': [float(x) for x in tnc_acc_test],\n",
    "    'test_auc_progression': [float(x) for x in tnc_auc_test],\n",
    "    'test_auprc_progression': [float(x) for x in tnc_auprc_test],\n",
    "    \n",
    "    # Confusion matrix\n",
    "    'confusion_matrix': c_mtx_enc.tolist()\n",
    "}\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "results_file = os.path.join(plots_dir, 'original_classification_results.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\\\nðŸ’¾ Complete results saved to: {results_file}\")\n",
    "\n",
    "# Print final summary exactly as original would show\n",
    "print(f\"\\\\n\" + \"=\"*80)\n",
    "print(f\"ðŸ“‹ ORIGINAL TNC CLASSIFICATION SUMMARY\")\n",
    "print(f\"=\"*80)\n",
    "print(f\"ðŸ“ Checkpoint: {checkpoint_file}\")\n",
    "print(f\"ðŸ“Š Data: {data} (CV={cv})\")\n",
    "print(f\"ðŸ”§ Method: WFClassificationExperiment (Original Codebase)\")\n",
    "print(f\"âš™ï¸  Encoding Size: {encoding_size}, Classes: {n_classes}\")\n",
    "print(f\"ðŸ“ Window Size: {window_size}\")\n",
    "print(f\"ðŸŽ“ Training Samples: {n_train:,}, Validation: {len(x_window) - n_train:,}\")\n",
    "print(f\"\\\\nðŸŽ¯ FINAL METRICS:\")\n",
    "print(f\"   â€¢ Accuracy: {tnc_acc_test[-1]:.4f} ({tnc_acc_test[-1]*100:.2f}%)\")\n",
    "print(f\"   â€¢ AUPRC: {tnc_auprc_test[-1]:.4f}\")\n",
    "print(f\"   â€¢ AUC: {tnc_auc_test[-1]:.4f}\")\n",
    "print(f\"   â€¢ Loss: {tnc_loss_test[-1]:.4f}\")\n",
    "print(f\"\\\\nâœ… Original method evaluation completed successfully!\")\n",
    "print(f\"ðŸ“ All results and plots saved to: {plots_dir}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bde35bc",
   "metadata": {},
   "source": [
    "## 8. Prototypical Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalNetwork(nn.Module):\n",
    "    \"\"\"Prototypical Network for few-shot learning with ECG data\"\"\"\n",
    "    def __init__(self, encoder, distance_metric='euclidean'):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.distance_metric = distance_metric\n",
    "        \n",
    "    def compute_prototypes(self, support_set, support_labels):\n",
    "        \"\"\"\n",
    "        Compute class prototypes from support set\n",
    "        Args:\n",
    "            support_set: [n_support, 2, 2500] ECG windows\n",
    "            support_labels: [n_support] class labels\n",
    "        Returns:\n",
    "            prototypes: [n_classes, encoding_size] class prototypes\n",
    "        \"\"\"\n",
    "        # Get encodings for support set\n",
    "        support_encodings = self.encoder(support_set)  # [n_support, 64]\n",
    "        \n",
    "        # Compute prototype for each class\n",
    "        unique_classes = torch.unique(support_labels)\n",
    "        prototypes = []\n",
    "        \n",
    "        for class_id in unique_classes:\n",
    "            # Get all examples of this class\n",
    "            class_mask = (support_labels == class_id)\n",
    "            class_encodings = support_encodings[class_mask]\n",
    "            \n",
    "            # Compute prototype as mean of class examples\n",
    "            prototype = class_encodings.mean(dim=0)\n",
    "            prototypes.append(prototype)\n",
    "            \n",
    "        return torch.stack(prototypes), unique_classes\n",
    "    \n",
    "    def compute_distances(self, query_encodings, prototypes):\n",
    "        \"\"\"Compute distances between queries and prototypes\"\"\"\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            # Euclidean distance: ||q - p||Â²\n",
    "            distances = torch.cdist(query_encodings, prototypes, p=2)\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            # Cosine distance: 1 - cosine_similarity\n",
    "            query_norm = F.normalize(query_encodings, dim=1)\n",
    "            proto_norm = F.normalize(prototypes, dim=1)\n",
    "            similarities = torch.mm(query_norm, proto_norm.t())\n",
    "            distances = 1 - similarities\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {self.distance_metric}\")\n",
    "            \n",
    "        return distances\n",
    "    \n",
    "    def forward(self, support_set, support_labels, query_set):\n",
    "        \"\"\"\n",
    "        Forward pass for prototypical network\n",
    "        Args:\n",
    "            support_set: [n_support, 2, 2500] support examples\n",
    "            support_labels: [n_support] support labels  \n",
    "            query_set: [n_query, 2, 2500] query examples\n",
    "        Returns:\n",
    "            logits: [n_query, n_classes] classification logits\n",
    "        \"\"\"\n",
    "        # Compute prototypes from support set\n",
    "        prototypes, class_ids = self.compute_prototypes(support_set, support_labels)\n",
    "        \n",
    "        # Get query encodings\n",
    "        query_encodings = self.encoder(query_set)\n",
    "        \n",
    "        # Compute distances\n",
    "        distances = self.compute_distances(query_encodings, prototypes)\n",
    "        \n",
    "        # Convert distances to logits (negative distance)\n",
    "        logits = -distances\n",
    "        \n",
    "        return logits, class_ids\n",
    "\n",
    "\n",
    "def create_few_shot_episode(x_window, y_window, n_way=4, k_shot=5, n_query=15):\n",
    "    \"\"\"\n",
    "    Create a few-shot learning episode from the ECG data\n",
    "    Args:\n",
    "        x_window: All ECG windows\n",
    "        y_window: All labels\n",
    "        n_way: Number of classes in episode\n",
    "        k_shot: Number of support examples per class\n",
    "        n_query: Number of query examples per class\n",
    "    \"\"\"\n",
    "    episode_support_x, episode_support_y = [], []\n",
    "    episode_query_x, episode_query_y = [], []\n",
    "    \n",
    "    # Get available classes\n",
    "    unique_classes = torch.unique(y_window)\n",
    "    \n",
    "    # Handle class imbalance - some classes have very few samples\n",
    "    available_classes = []\n",
    "    for class_id in unique_classes:\n",
    "        class_count = (y_window == class_id).sum().item()\n",
    "        if class_count >= k_shot + n_query:  # Need enough samples\n",
    "            available_classes.append(class_id)\n",
    "    \n",
    "    # Select n_way classes (or all available if less than n_way)\n",
    "    selected_classes = available_classes[:min(n_way, len(available_classes))]\n",
    "    \n",
    "    for class_id in selected_classes:\n",
    "        # Get all examples of this class\n",
    "        class_indices = torch.where(y_window == class_id)[0]\n",
    "        \n",
    "        # Randomly sample k_shot + n_query examples\n",
    "        perm = torch.randperm(len(class_indices))\n",
    "        selected_indices = class_indices[perm[:k_shot + n_query]]\n",
    "        \n",
    "        # Split into support and query\n",
    "        support_indices = selected_indices[:k_shot]\n",
    "        query_indices = selected_indices[k_shot:k_shot + n_query]\n",
    "        \n",
    "        # Add to episode\n",
    "        episode_support_x.append(x_window[support_indices])\n",
    "        episode_support_y.append(y_window[support_indices])\n",
    "        episode_query_x.append(x_window[query_indices])\n",
    "        episode_query_y.append(y_window[query_indices])\n",
    "    \n",
    "    # Concatenate all classes\n",
    "    support_x = torch.cat(episode_support_x, dim=0)\n",
    "    support_y = torch.cat(episode_support_y, dim=0)\n",
    "    query_x = torch.cat(episode_query_x, dim=0)\n",
    "    query_y = torch.cat(episode_query_y, dim=0)\n",
    "    \n",
    "    return support_x, support_y, query_x, query_y, selected_classes\n",
    "\n",
    "\n",
    "# Initialize prototypical network\n",
    "proto_net = PrototypicalNetwork(encoder, distance_metric='euclidean')\n",
    "print(\"Prototypical Network initialized!\")\n",
    "\n",
    "# Test with a few-shot episode\n",
    "print(\"\\\\nTesting prototypical network with imbalanced data...\")\n",
    "\n",
    "# Create episode considering class imbalance\n",
    "support_x, support_y, query_x, query_y, episode_classes = create_few_shot_episode(\n",
    "    x_window, y_window, n_way=4, k_shot=3, n_query=10\n",
    ")\n",
    "\n",
    "print(f\"Episode classes: {episode_classes}\")\n",
    "print(f\"Support set: {support_x.shape}, labels: {support_y.shape}\")\n",
    "print(f\"Query set: {query_x.shape}, labels: {query_y.shape}\")\n",
    "print(f\"Support class distribution: {torch.bincount(support_y.long())}\")\n",
    "print(f\"Query class distribution: {torch.bincount(query_y.long())}\")\n",
    "\n",
    "# Move to device\n",
    "support_x, support_y = support_x.to(device), support_y.to(device)\n",
    "query_x, query_y = query_x.to(device), query_y.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    logits, class_ids = proto_net(support_x, support_y, query_x)\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # Map predictions back to original class IDs\n",
    "    pred_classes = class_ids[predictions]\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (pred_classes == query_y).float().mean()\n",
    "    \n",
    "print(f\"\\\\nPrototypical Network Results:\")\n",
    "print(f\"Query accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"Predicted classes: {pred_classes}\")\n",
    "print(f\"True classes: {query_y}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ KEY INSIGHTS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸ“Š Your dataset has severe class imbalance:\")\n",
    "print(f\"   â€¢ Class 0: 24,394 samples (abundant)\")\n",
    "print(f\"   â€¢ Class 1: 247 samples (few-shot)\")  \n",
    "print(f\"   â€¢ Class 2: 31 samples (extreme few-shot)\")\n",
    "print(f\"   â€¢ Class 3: 35,250 samples (abundant)\")\n",
    "print(f\"\\\\nðŸ”¬ Prototypical networks help by:\")\n",
    "print(f\"   â€¢ Better handling of rare classes (1 & 2)\")\n",
    "print(f\"   â€¢ Distance-based classification instead of linear\")\n",
    "print(f\"   â€¢ Can work with very few examples per class\")\n",
    "print(f\"   â€¢ More robust to class imbalance\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21983e2f",
   "metadata": {},
   "source": [
    "## 9. Compare Linear vs Prototypical Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prototypical_network(encoder, x_data, y_data, n_episodes=100, k_shot=5, n_query=15):\n",
    "    \"\"\"\n",
    "    Evaluate prototypical network performance across multiple episodes\n",
    "    \"\"\"\n",
    "    proto_net = PrototypicalNetwork(encoder, distance_metric='euclidean')\n",
    "    proto_net.eval()\n",
    "    \n",
    "    accuracies = []\n",
    "    class_accuracies = {i: [] for i in range(4)}\n",
    "    \n",
    "    print(f\"Evaluating prototypical network over {n_episodes} episodes...\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        try:\n",
    "            # Create episode\n",
    "            support_x, support_y, query_x, query_y, episode_classes = create_few_shot_episode(\n",
    "                x_data, y_data, n_way=4, k_shot=k_shot, n_query=n_query\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            support_x = support_x.to(device)\n",
    "            support_y = support_y.to(device) \n",
    "            query_x = query_x.to(device)\n",
    "            query_y = query_y.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                logits, class_ids = proto_net(support_x, support_y, query_x)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                pred_classes = class_ids[predictions]\n",
    "                \n",
    "                # Overall accuracy\n",
    "                accuracy = (pred_classes == query_y).float().mean().item()\n",
    "                accuracies.append(accuracy)\n",
    "                \n",
    "                # Per-class accuracy\n",
    "                for class_id in episode_classes:\n",
    "                    class_mask = (query_y == class_id)\n",
    "                    if class_mask.sum() > 0:\n",
    "                        class_acc = (pred_classes[class_mask] == query_y[class_mask]).float().mean().item()\n",
    "                        class_accuracies[class_id.item()].append(class_acc)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Episode {episode} failed: {e}\")\n",
    "            continue\n",
    "            \n",
    "        if (episode + 1) % 20 == 0:\n",
    "            print(f\"Episode {episode + 1}/{n_episodes}, Mean accuracy: {np.mean(accuracies):.3f}\")\n",
    "    \n",
    "    return accuracies, class_accuracies\n",
    "\n",
    "# Evaluate prototypical network\n",
    "print(\"ðŸš€ Starting prototypical network evaluation...\")\n",
    "proto_accuracies, proto_class_accs = evaluate_prototypical_network(\n",
    "    encoder, x_window, y_window, n_episodes=50, k_shot=3, n_query=10\n",
    ")\n",
    "\n",
    "# Calculate statistics\n",
    "proto_mean_acc = np.mean(proto_accuracies)\n",
    "proto_std_acc = np.std(proto_accuracies)\n",
    "\n",
    "print(f\"\\\\n\" + \"=\"*70)\n",
    "print(f\"ðŸ“Š PROTOTYPICAL NETWORK RESULTS\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"ðŸ“ˆ Overall Accuracy: {proto_mean_acc:.3f} Â± {proto_std_acc:.3f}\")\n",
    "print(f\"ðŸ“ˆ Overall Accuracy: {proto_mean_acc*100:.1f}% Â± {proto_std_acc*100:.1f}%\")\n",
    "\n",
    "print(f\"\\\\nðŸ“‹ Per-Class Performance:\")\n",
    "for class_id, class_accs in proto_class_accs.items():\n",
    "    if class_accs:\n",
    "        class_mean = np.mean(class_accs)\n",
    "        class_std = np.std(class_accs)\n",
    "        class_count = (y_window == class_id).sum().item()\n",
    "        print(f\"   â€¢ Class {class_id}: {class_mean:.3f} Â± {class_std:.3f} ({class_mean*100:.1f}% Â± {class_std*100:.1f}%) [{class_count:,} total samples]\")\n",
    "    else:\n",
    "        print(f\"   â€¢ Class {class_id}: No episodes (insufficient samples)\")\n",
    "\n",
    "# Compare with linear classifier results\n",
    "print(f\"\\\\n\" + \"=\"*70)\n",
    "print(f\"ðŸ”„ COMPARISON: Linear vs Prototypical\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"ðŸ“Œ Linear Classifier:\")\n",
    "print(f\"   â€¢ Test Accuracy: {tnc_acc_test[-1]:.3f} ({tnc_acc_test[-1]*100:.1f}%)\")\n",
    "print(f\"   â€¢ Method: Traditional supervised learning\")\n",
    "print(f\"   â€¢ Training: 35,953 samples\")\n",
    "print(f\"\\\\nðŸ“Œ Prototypical Network:\")\n",
    "print(f\"   â€¢ Test Accuracy: {proto_mean_acc:.3f} Â± {proto_std_acc:.3f}\")\n",
    "print(f\"   â€¢ Method: Few-shot learning with {k_shot}-shot episodes\")\n",
    "print(f\"   â€¢ Training: {k_shot} samples per class per episode\")\n",
    "\n",
    "print(f\"\\\\nðŸ’¡ Key Advantages of Prototypical Networks:\")\n",
    "print(f\"   âœ… Better handling of class imbalance\")\n",
    "print(f\"   âœ… Works with very few examples (3-shot learning)\")\n",
    "print(f\"   âœ… Distance-based similarity matching\")\n",
    "print(f\"   âœ… Can adapt to new patients/conditions quickly\")\n",
    "print(f\"   âœ… More robust to rare ECG patterns\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
