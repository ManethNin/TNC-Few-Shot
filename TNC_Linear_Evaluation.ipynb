{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea95a1de",
   "metadata": {},
   "source": [
    "# TNC Linear Evaluation Notebook\n",
    "\n",
    "This notebook evaluates a pre-trained TNC encoder using linear classification on ECG waveform data. We measure both AUPRC (Area Under Precision-Recall Curve) and Accuracy to assess the quality of learned representations.\n",
    "\n",
    "## Overview\n",
    "1. Load pre-trained TNC encoder from checkpoint\n",
    "2. Extract features from ECG waveform data\n",
    "3. Train linear classifier on extracted features\n",
    "4. Evaluate performance with AUPRC and Accuracy metrics\n",
    "5. Visualize results with confusion matrix and precision-recall curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f892bf2a",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive and Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up paths to your saved checkpoint, data, and plots folders\n",
    "DRIVE_PATH = '/content/drive/MyDrive'  # Adjust this path as needed\n",
    "CHECKPOINT_PATH = os.path.join(DRIVE_PATH, 'ckpt')\n",
    "DATA_PATH = os.path.join(DRIVE_PATH, 'data')\n",
    "PLOTS_PATH = os.path.join(DRIVE_PATH, 'plots')\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "os.makedirs(PLOTS_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Plots path: {PLOTS_PATH}\")\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Checkpoint exists: {os.path.exists(CHECKPOINT_PATH)}\")\n",
    "print(f\"Data exists: {os.path.exists(DATA_PATH)}\")\n",
    "print(f\"Plots exists: {os.path.exists(PLOTS_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbcd4ad",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_recall_curve, \n",
    "                           average_precision_score, roc_auc_score,\n",
    "                           confusion_matrix, classification_report)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9509e2e",
   "metadata": {},
   "source": [
    "## 3. Define TNC Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5346f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WFEncoder(nn.Module):\n",
    "    \"\"\"TNC Waveform Encoder for ECG data\"\"\"\n",
    "    def __init__(self, encoding_size=64, classify=False, n_classes=None):\n",
    "        super(WFEncoder, self).__init__()\n",
    "        \n",
    "        self.encoding_size = encoding_size\n",
    "        self.n_classes = n_classes\n",
    "        self.classify = classify\n",
    "        self.classifier = None\n",
    "        \n",
    "        if self.classify:\n",
    "            if self.n_classes is None:\n",
    "                raise ValueError('Need to specify the number of output classes for the encoder')\n",
    "            else:\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(self.encoding_size, self.n_classes)\n",
    "                )\n",
    "                nn.init.xavier_uniform_(self.classifier[1].weight)\n",
    "\n",
    "        # Convolutional feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(2, 64, kernel_size=4, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(64, eps=0.001),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(64, eps=0.001),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(128, eps=0.001),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(128, eps=0.001),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(256, eps=0.001),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(256, eps=0.001),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(79872, 2048),  # Adjust based on your input size\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(2048, eps=0.001),\n",
    "            nn.Linear(2048, self.encoding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        encoding = self.fc(x)\n",
    "        \n",
    "        if self.classify:\n",
    "            c = self.classifier(encoding)\n",
    "            return c\n",
    "        else:\n",
    "            return encoding\n",
    "\n",
    "print(\"TNC WFEncoder model defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2232e8d5",
   "metadata": {},
   "source": [
    "## 4. Load Trained Encoder from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca776fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained TNC encoder\n",
    "checkpoint_file = os.path.join(CHECKPOINT_PATH, 'waveform', 'checkpoint_0.pth.tar')\n",
    "\n",
    "print(f\"Loading checkpoint from: {checkpoint_file}\")\n",
    "print(f\"Checkpoint exists: {os.path.exists(checkpoint_file)}\")\n",
    "\n",
    "if not os.path.exists(checkpoint_file):\n",
    "    print(\"ERROR: Checkpoint file not found!\")\n",
    "    print(\"Make sure your checkpoint is saved as: ckpt/waveform/checkpoint_0.pth.tar\")\n",
    "    print(\"Available files in checkpoint directory:\")\n",
    "    if os.path.exists(os.path.join(CHECKPOINT_PATH, 'waveform')):\n",
    "        print(os.listdir(os.path.join(CHECKPOINT_PATH, 'waveform')))\n",
    "    else:\n",
    "        print(\"Waveform directory doesn't exist\")\n",
    "else:\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "    print(f\"Checkpoint loaded successfully!\")\n",
    "    print(f\"Available keys in checkpoint: {list(checkpoint.keys())}\")\n",
    "    \n",
    "    # Initialize encoder\n",
    "    encoder = WFEncoder(encoding_size=64)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    encoder = encoder.to(device)\n",
    "    encoder.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(\"Encoder loaded and set to evaluation mode!\")\n",
    "    print(f\"Encoder is on device: {next(encoder.parameters()).device}\")\n",
    "    \n",
    "    # Print some checkpoint info if available\n",
    "    if 'best_accuracy' in checkpoint:\n",
    "        print(f\"Best training accuracy: {checkpoint['best_accuracy']:.3f}\")\n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"Training epoch: {checkpoint['epoch']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d99b1",
   "metadata": {},
   "source": [
    "## 5. Load and Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ECG waveform data\n",
    "wf_datapath = os.path.join(DATA_PATH, 'waveform_data', 'processed')\n",
    "\n",
    "# Check if data files exist\n",
    "x_train_file = os.path.join(wf_datapath, 'x_train.pkl')\n",
    "y_train_file = os.path.join(wf_datapath, 'state_train.pkl')\n",
    "x_test_file = os.path.join(wf_datapath, 'x_test.pkl')\n",
    "y_test_file = os.path.join(wf_datapath, 'state_test.pkl')\n",
    "\n",
    "print(f\"Data directory: {wf_datapath}\")\n",
    "print(f\"x_train exists: {os.path.exists(x_train_file)}\")\n",
    "print(f\"y_train exists: {os.path.exists(y_train_file)}\")\n",
    "print(f\"x_test exists: {os.path.exists(x_test_file)}\")\n",
    "print(f\"y_test exists: {os.path.exists(y_test_file)}\")\n",
    "\n",
    "# Load training data for evaluation\n",
    "try:\n",
    "    with open(x_train_file, 'rb') as f:\n",
    "        x_train = pickle.load(f)\n",
    "    with open(y_train_file, 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "    \n",
    "    print(f\"Training data loaded successfully!\")\n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    # Also load test data if available\n",
    "    if os.path.exists(x_test_file) and os.path.exists(y_test_file):\n",
    "        with open(x_test_file, 'rb') as f:\n",
    "            x_test = pickle.load(f)\n",
    "        with open(y_test_file, 'rb') as f:\n",
    "            y_test = pickle.load(f)\n",
    "        print(f\"Test data loaded successfully!\")\n",
    "        print(f\"x_test shape: {x_test.shape}\")\n",
    "        print(f\"y_test shape: {y_test.shape}\")\n",
    "    else:\n",
    "        print(\"Test data not found, will use train data split\")\n",
    "        x_test, y_test = None, None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please check your data file paths and formats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0996dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for linear classification\n",
    "window_size = 2500  # Standard window size for waveform data\n",
    "\n",
    "def prepare_windowed_data(x_data, y_data, window_size):\n",
    "    \"\"\"Convert continuous data into windowed segments\"\"\"\n",
    "    T = x_data.shape[-1]\n",
    "    n_windows = T // window_size\n",
    "    \n",
    "    # Reshape into windows\n",
    "    x_windowed = np.split(x_data[:, :, :window_size * n_windows], n_windows, -1)\n",
    "    y_windowed = np.split(y_data[:, :window_size * n_windows], n_windows, -1)\n",
    "    \n",
    "    # Concatenate all windows\n",
    "    x_windowed = np.concatenate(x_windowed, 0)\n",
    "    y_windowed = np.concatenate(y_windowed, 0)\n",
    "    \n",
    "    # Get majority vote for each window\n",
    "    y_windowed = np.array([np.bincount(yy.astype(int)).argmax() for yy in y_windowed])\n",
    "    \n",
    "    return x_windowed, y_windowed\n",
    "\n",
    "# Prepare training data\n",
    "x_train_windowed, y_train_windowed = prepare_windowed_data(x_train, y_train, window_size)\n",
    "\n",
    "print(f\"Windowed training data shape: {x_train_windowed.shape}\")\n",
    "print(f\"Windowed training labels shape: {y_train_windowed.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train_windowed))}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train_windowed.astype(int))}\")\n",
    "\n",
    "# Prepare test data if available\n",
    "if x_test is not None and y_test is not None:\n",
    "    x_test_windowed, y_test_windowed = prepare_windowed_data(x_test, y_test, window_size)\n",
    "    print(f\"Windowed test data shape: {x_test_windowed.shape}\")\n",
    "    print(f\"Windowed test labels shape: {y_test_windowed.shape}\")\n",
    "else:\n",
    "    # Split training data for evaluation\n",
    "    split_idx = int(0.7 * len(x_train_windowed))\n",
    "    x_test_windowed = x_train_windowed[split_idx:]\n",
    "    y_test_windowed = y_train_windowed[split_idx:]\n",
    "    x_train_windowed = x_train_windowed[:split_idx]\n",
    "    y_train_windowed = y_train_windowed[:split_idx]\n",
    "    \n",
    "    print(f\"Split data - Train: {x_train_windowed.shape}, Test: {x_test_windowed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89b4b8",
   "metadata": {},
   "source": [
    "## 6. Extract Features Using Trained Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(encoder, data, batch_size=32):\n",
    "    \"\"\"Extract features using the trained encoder\"\"\"\n",
    "    encoder.eval()\n",
    "    features_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch = data[i:i+batch_size]\n",
    "            batch_tensor = torch.FloatTensor(batch).to(device)\n",
    "            features = encoder(batch_tensor)\n",
    "            features_list.append(features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(features_list)\n",
    "\n",
    "print(\"Extracting features from training data...\")\n",
    "train_features = extract_features(encoder, x_train_windowed)\n",
    "\n",
    "print(\"Extracting features from test data...\")\n",
    "test_features = extract_features(encoder, x_test_windowed)\n",
    "\n",
    "print(f\"Training features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "print(f\"Feature dimension: {train_features.shape[1]}\")\n",
    "\n",
    "# Check for any NaN or infinite values\n",
    "print(f\"Training features - NaN count: {np.isnan(train_features).sum()}\")\n",
    "print(f\"Training features - Inf count: {np.isinf(train_features).sum()}\")\n",
    "print(f\"Test features - NaN count: {np.isnan(test_features).sum()}\")\n",
    "print(f\"Test features - Inf count: {np.isinf(test_features).sum()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"Training features - Min: {train_features.min():.3f}, Max: {train_features.max():.3f}\")\n",
    "print(f\"Training features - Mean: {train_features.mean():.3f}, Std: {train_features.std():.3f}\")\n",
    "\n",
    "# Visualize feature distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_features.flatten(), bins=50, alpha=0.7, label='Training Features')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Extracted Features')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_features[:100].T, alpha=0.3)\n",
    "plt.xlabel('Feature Dimension')\n",
    "plt.ylabel('Feature Value')\n",
    "plt.title('Feature Vectors (First 100 samples)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOTS_PATH, 'feature_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5975d2fb",
   "metadata": {},
   "source": [
    "## 7. Train Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f69bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features for better classifier performance\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "print(f\"Features normalized - Train mean: {train_features_scaled.mean():.3f}, Test mean: {test_features_scaled.mean():.3f}\")\n",
    "\n",
    "# Train linear classifier (Logistic Regression)\n",
    "print(\"Training linear classifier...\")\n",
    "\n",
    "# Try different regularization values to find the best one\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "best_score = 0\n",
    "best_C = 1.0\n",
    "\n",
    "for C in C_values:\n",
    "    classifier = LogisticRegression(C=C, max_iter=1000, random_state=42, multi_class='ovr')\n",
    "    scores = cross_val_score(classifier, train_features_scaled, y_train_windowed, cv=5, scoring='accuracy')\n",
    "    mean_score = scores.mean()\n",
    "    print(f\"C={C}: CV Accuracy = {mean_score:.4f} Â± {scores.std():.4f}\")\n",
    "    \n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_C = C\n",
    "\n",
    "print(f\"\\\\nBest regularization parameter: C = {best_C}\")\n",
    "print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Train final classifier with best parameters\n",
    "final_classifier = LogisticRegression(C=best_C, max_iter=1000, random_state=42, multi_class='ovr')\n",
    "final_classifier.fit(train_features_scaled, y_train_windowed)\n",
    "\n",
    "print(\"Linear classifier training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e31120",
   "metadata": {},
   "source": [
    "## 8. Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359fc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"Making predictions on test set...\")\n",
    "\n",
    "# Get predicted labels\n",
    "y_pred = final_classifier.predict(test_features_scaled)\n",
    "\n",
    "# Get prediction probabilities for AUPRC calculation\n",
    "y_pred_proba = final_classifier.predict_proba(test_features_scaled)\n",
    "\n",
    "print(f\"Predictions completed!\")\n",
    "print(f\"Test set size: {len(y_test_windowed)}\")\n",
    "print(f\"Prediction shape: {y_pred.shape}\")\n",
    "print(f\"Probability shape: {y_pred_proba.shape}\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(f\"\\\\nPrediction distribution:\")\n",
    "unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "for class_idx, count in zip(unique_pred, counts_pred):\n",
    "    print(f\"Class {class_idx}: {count} predictions ({count/len(y_pred)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\\\nTrue label distribution:\")\n",
    "unique_true, counts_true = np.unique(y_test_windowed, return_counts=True)\n",
    "for class_idx, count in zip(unique_true, counts_true):\n",
    "    print(f\"Class {class_idx}: {count} samples ({count/len(y_test_windowed)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65059881",
   "metadata": {},
   "source": [
    "## 9. Calculate AUPRC and Accuracy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "print(\"=== PERFORMANCE EVALUATION ===\\\\n\")\n",
    "\n",
    "# 1. Accuracy Score\n",
    "accuracy = accuracy_score(y_test_windowed, y_pred)\n",
    "print(f\"ðŸŽ¯ ACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# 2. AUPRC (Area Under Precision-Recall Curve)\n",
    "n_classes = len(np.unique(y_test_windowed))\n",
    "\n",
    "if n_classes == 2:\n",
    "    # Binary classification\n",
    "    auprc = average_precision_score(y_test_windowed, y_pred_proba[:, 1])\n",
    "    print(f\"ðŸ“ˆ AUPRC (Binary): {auprc:.4f}\")\n",
    "else:\n",
    "    # Multi-class classification - calculate macro and micro averages\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from sklearn.metrics import precision_recall_curve, auc\n",
    "    \n",
    "    # Binarize labels for multi-class AUPRC\n",
    "    y_test_bin = label_binarize(y_test_windowed, classes=range(n_classes))\n",
    "    \n",
    "    # Micro-average AUPRC\n",
    "    auprc_micro = average_precision_score(y_test_bin, y_pred_proba, average='micro')\n",
    "    \n",
    "    # Macro-average AUPRC\n",
    "    auprc_macro = average_precision_score(y_test_bin, y_pred_proba, average='macro')\n",
    "    \n",
    "    # Per-class AUPRC\n",
    "    auprc_per_class = []\n",
    "    for i in range(n_classes):\n",
    "        if i < y_pred_proba.shape[1]:  # Check if class exists in predictions\n",
    "            auprc_class = average_precision_score(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "            auprc_per_class.append(auprc_class)\n",
    "        else:\n",
    "            auprc_per_class.append(0.0)\n",
    "    \n",
    "    print(f\"ðŸ“ˆ AUPRC (Micro-avg): {auprc_micro:.4f}\")\n",
    "    print(f\"ðŸ“ˆ AUPRC (Macro-avg): {auprc_macro:.4f}\")\n",
    "    \n",
    "    for i, auprc_val in enumerate(auprc_per_class):\n",
    "        print(f\"ðŸ“ˆ AUPRC Class {i}: {auprc_val:.4f}\")\n",
    "\n",
    "# 3. Additional metrics\n",
    "print(f\"\\\\n=== ADDITIONAL METRICS ===\")\n",
    "\n",
    "# ROC AUC (if applicable)\n",
    "try:\n",
    "    if n_classes == 2:\n",
    "        roc_auc = roc_auc_score(y_test_windowed, y_pred_proba[:, 1])\n",
    "        print(f\"ðŸ”„ ROC AUC (Binary): {roc_auc:.4f}\")\n",
    "    else:\n",
    "        roc_auc_micro = roc_auc_score(y_test_bin, y_pred_proba, average='micro', multi_class='ovr')\n",
    "        roc_auc_macro = roc_auc_score(y_test_bin, y_pred_proba, average='macro', multi_class='ovr')\n",
    "        print(f\"ðŸ”„ ROC AUC (Micro-avg): {roc_auc_micro:.4f}\")\n",
    "        print(f\"ðŸ”„ ROC AUC (Macro-avg): {roc_auc_macro:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not calculate ROC AUC: {e}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\\\n=== CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_test_windowed, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b0882",
   "metadata": {},
   "source": [
    "## 10. Visualize Results and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d715ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "cm = confusion_matrix(y_test_windowed, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
    "axes[0,0].set_title('Confusion Matrix')\n",
    "axes[0,0].set_xlabel('Predicted Label')\n",
    "axes[0,0].set_ylabel('True Label')\n",
    "\n",
    "# 2. Precision-Recall Curves\n",
    "if n_classes == 2:\n",
    "    # Binary case\n",
    "    precision, recall, _ = precision_recall_curve(y_test_windowed, y_pred_proba[:, 1])\n",
    "    axes[0,1].plot(recall, precision, linewidth=2, label=f'AUPRC = {auprc:.3f}')\n",
    "    axes[0,1].set_xlabel('Recall')\n",
    "    axes[0,1].set_ylabel('Precision')\n",
    "    axes[0,1].set_title('Precision-Recall Curve')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    # Multi-class case - show curves for each class\n",
    "    for i in range(min(n_classes, y_pred_proba.shape[1])):\n",
    "        precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "        axes[0,1].plot(recall, precision, linewidth=2, \n",
    "                      label=f'Class {i} (AUPRC = {auprc_per_class[i]:.3f})')\n",
    "    axes[0,1].set_xlabel('Recall')\n",
    "    axes[0,1].set_ylabel('Precision')\n",
    "    axes[0,1].set_title('Precision-Recall Curves (Multi-class)')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Class Distribution Comparison\n",
    "x_pos = np.arange(n_classes)\n",
    "width = 0.35\n",
    "\n",
    "true_counts = [np.sum(y_test_windowed == i) for i in range(n_classes)]\n",
    "pred_counts = [np.sum(y_pred == i) for i in range(n_classes)]\n",
    "\n",
    "axes[1,0].bar(x_pos - width/2, true_counts, width, label='True', alpha=0.8)\n",
    "axes[1,0].bar(x_pos + width/2, pred_counts, width, label='Predicted', alpha=0.8)\n",
    "axes[1,0].set_xlabel('Class')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].set_title('True vs Predicted Class Distribution')\n",
    "axes[1,0].set_xticks(x_pos)\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Performance Metrics Summary\n",
    "metrics_names = ['Accuracy']\n",
    "metrics_values = [accuracy]\n",
    "\n",
    "if n_classes == 2:\n",
    "    metrics_names.extend(['AUPRC', 'ROC AUC'])\n",
    "    metrics_values.extend([auprc, roc_auc if 'roc_auc' in locals() else 0])\n",
    "else:\n",
    "    metrics_names.extend(['AUPRC (Micro)', 'AUPRC (Macro)'])\n",
    "    metrics_values.extend([auprc_micro, auprc_macro])\n",
    "\n",
    "bars = axes[1,1].bar(metrics_names, metrics_values, color=['skyblue', 'lightcoral', 'lightgreen'][:len(metrics_values)])\n",
    "axes[1,1].set_ylabel('Score')\n",
    "axes[1,1].set_title('Performance Metrics Summary')\n",
    "axes[1,1].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOTS_PATH, 'linear_evaluation_results.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\\\nðŸ“Š Plots saved to: {PLOTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to file\n",
    "results_summary = {\n",
    "    'model': 'TNC_Linear_Evaluation',\n",
    "    'encoder_checkpoint': checkpoint_file,\n",
    "    'test_samples': len(y_test_windowed),\n",
    "    'feature_dimension': train_features.shape[1],\n",
    "    'n_classes': n_classes,\n",
    "    'accuracy': float(accuracy),\n",
    "    'best_regularization_C': float(best_C),\n",
    "    'cross_val_accuracy': float(best_score)\n",
    "}\n",
    "\n",
    "if n_classes == 2:\n",
    "    results_summary['auprc'] = float(auprc)\n",
    "    if 'roc_auc' in locals():\n",
    "        results_summary['roc_auc'] = float(roc_auc)\n",
    "else:\n",
    "    results_summary['auprc_micro'] = float(auprc_micro)\n",
    "    results_summary['auprc_macro'] = float(auprc_macro)\n",
    "    results_summary['auprc_per_class'] = [float(x) for x in auprc_per_class]\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "results_file = os.path.join(PLOTS_PATH, 'linear_evaluation_results.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\\\nðŸ’¾ Results saved to: {results_file}\")\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\\\n\" + \"=\"*50)\n",
    "print(f\"ðŸŽ¯ FINAL EVALUATION SUMMARY\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Model: TNC Linear Evaluation\")\n",
    "print(f\"Test Samples: {len(y_test_windowed):,}\")\n",
    "print(f\"Feature Dimension: {train_features.shape[1]}\")\n",
    "print(f\"Number of Classes: {n_classes}\")\n",
    "print(f\"\\\\nðŸ“Š MAIN METRICS:\")\n",
    "print(f\"   â€¢ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "if n_classes == 2:\n",
    "    print(f\"   â€¢ AUPRC: {auprc:.4f}\")\n",
    "else:\n",
    "    print(f\"   â€¢ AUPRC (Micro): {auprc_micro:.4f}\")\n",
    "    print(f\"   â€¢ AUPRC (Macro): {auprc_macro:.4f}\")\n",
    "\n",
    "print(f\"\\\\nâœ… Evaluation completed successfully!\")\n",
    "print(f\"ðŸ“ All results and plots saved to: {PLOTS_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
